{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Yonaguni transcriptions for MFA forced aligner\n",
    "- This script processes Yonaguni transcriptions from ELAN in the following ways:\n",
    "    \n",
    "    - Modifies the ELAN transcriptions .csv file into a dataframe with start and end times and MFA transcriptions for each annotation\n",
    "\n",
    "    - Moves the transcriptions that have the target phonemes to a folder for alignment\n",
    "\n",
    "    - Fills in the textgrids automatically with MFA friendly transcriptions\n",
    "\n",
    "    - Builds a dictionary mapping the transcriptions the MFA can process\n",
    "     \n",
    "- Before starting this script, you should\n",
    "\n",
    "    1) Have your ELAN file fully annotated. If you have empty annotations segmented out, fill them in with \"xxx\" for now\n",
    "    \n",
    "    2) In ELAN, export your .eaf file as a Praat .TextGrid and as a .csv file (Tab Delimited is the option under File)\n",
    "    \n",
    "    3) Load the whole .wav file as a LongSound File and the .TextGrid file you just exported into Praat \n",
    "    \n",
    "    4) Make a clips folder and use the save_intervals_to_wav_sound_files.praat script (skip empty annotations) to cut the .wav file into the annotated chunks\n",
    "    \n",
    "    5) This should produce a folder with the clipped .wav files numbered 1 through however many annotations you have + a .txt file with all the annotations\n",
    "\n",
    "- Now name the following folders\n",
    "\n",
    "    - working_folder : the overall folder you are working with\n",
    "    - clipped_folder : folder where you have the original clipped files from the script\n",
    "    - target_folder : folder for the files with the target segments to be moved to\n",
    "    \n",
    "## You can scroll down to the bottom to see a screenshot of what the folders could look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_folder = 'YO0008c_yoneshiro_sosensuuhai_20130410'\n",
    "clipped_folder = working_folder + '/clips'\n",
    "target_folder = working_folder + '/targets-stereo'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import nested_scopes\n",
    "\n",
    "import pandas as pd\n",
    "import glob, os\n",
    "import numpy as np\n",
    "import audiolabel as al\n",
    "import re\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_replace(adict, text):\n",
    "    '''Takes a dictionary and a string and replaces keys with values'''\n",
    "    # Create a regular expression from all of the dictionary keys\n",
    "    regex = re.compile(\"|\".join(map(re.escape, adict.keys(  ))))\n",
    "    \n",
    "    # For each match, look up the corresponding value in the dictionary\n",
    "    return regex.sub(lambda match: adict[match.group(0)], text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in file that we are managing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Begin Time - msec</th>\n",
       "      <th>End Time - msec</th>\n",
       "      <th>Yonaguni</th>\n",
       "      <th>fileno</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>120865</td>\n",
       "      <td>127366</td>\n",
       "      <td>hata=nki m dama=nkí iti=ti (n tti) timunu sai ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>127800</td>\n",
       "      <td>133940</td>\n",
       "      <td>ndi=nki maru munu ni hamiru=ndí nta=ba unu ang...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>134285</td>\n",
       "      <td>138166</td>\n",
       "      <td>da=gara tundiru munu dama=nki=ja hiranu=ti umi</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>139380</td>\n",
       "      <td>147611</td>\n",
       "      <td>aci-taburu=ndi taburu=du a=nga ata=ba umi udi ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>148651</td>\n",
       "      <td>157866</td>\n",
       "      <td>ja ubuda=nga nda=ja ba-nta=ja=ju ttui=ja h-i=d...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Begin Time - msec  End Time - msec  \\\n",
       "0             120865           127366   \n",
       "1             127800           133940   \n",
       "2             134285           138166   \n",
       "3             139380           147611   \n",
       "4             148651           157866   \n",
       "\n",
       "                                            Yonaguni fileno  \n",
       "0  hata=nki m dama=nkí iti=ti (n tti) timunu sai ...      1  \n",
       "1  ndi=nki maru munu ni hamiru=ndí nta=ba unu ang...      2  \n",
       "2     da=gara tundiru munu dama=nki=ja hiranu=ti umi      3  \n",
       "3  aci-taburu=ndi taburu=du a=nga ata=ba umi udi ...      4  \n",
       "4  ja ubuda=nga nda=ja ba-nta=ja=ju ttui=ja h-i=d...      5  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = working_folder + '/' + working_folder + '.csv'\n",
    "\n",
    "df = pd.read_csv(f, keep_default_na = False)\n",
    "\n",
    "# Drop unnamed columns\n",
    "df = df.drop([c for c in df.columns if 'Unnamed' in c], axis = 1)\n",
    "\n",
    "df['fileno'] = df.index + 1\n",
    "df['fileno'] = df['fileno'].apply(str)\n",
    "\n",
    "# Add column for number corresponding to file number\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'YS_Transcription-txt-rys' in df.columns:\n",
    "    df = df.rename(columns = {'YS_Transcription-txt-rys':'Yonaguni'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only want files that begin with these phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ['p', 't', 'k', 'c', 'n', 'm', 'ts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_first(sentence):\n",
    "    '''Breaks up the sentence by spaces and looks at the first letter for each word'''\n",
    "    # Get rid of Japanese code switching or uncertainties (parentheses)\n",
    "    sentence_nojp = re.sub('<[^(.+>.+<)]+>', '', sentence)\n",
    "    sentence_nojp = re.sub('\\([^(.+\\).+\\()]+\\)', '', sentence_nojp)\n",
    "    \n",
    "    # Look at the first letter in each word. Ignore n\n",
    "    first_letters = [w[0] for w in sentence_nojp.split() if len(w) > 1]\n",
    "    # And first 2, in case ts\n",
    "    first_digraph = [w[0:2] for w in sentence_nojp.split() if len(w) > 1]\n",
    "    \n",
    "    # Only take those that are overlapped\n",
    "    inter = set.intersection(set(first_letters), set(targets))\n",
    "    \n",
    "    return(len(inter) > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply function to subset dataframe to just those tokens that contain our targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Begin Time - msec</th>\n",
       "      <th>End Time - msec</th>\n",
       "      <th>Yonaguni</th>\n",
       "      <th>fileno</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>120865</td>\n",
       "      <td>127366</td>\n",
       "      <td>hata=nki m dama=nkí iti=ti (n tti) timunu sai ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>127800</td>\n",
       "      <td>133940</td>\n",
       "      <td>ndi=nki maru munu ni hamiru=ndí nta=ba unu ang...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>134285</td>\n",
       "      <td>138166</td>\n",
       "      <td>da=gara tundiru munu dama=nki=ja hiranu=ti umi</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>139380</td>\n",
       "      <td>147611</td>\n",
       "      <td>aci-taburu=ndi taburu=du a=nga ata=ba umi udi ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>148651</td>\n",
       "      <td>157866</td>\n",
       "      <td>ja ubuda=nga nda=ja ba-nta=ja=ju ttui=ja h-i=d...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Begin Time - msec  End Time - msec  \\\n",
       "0             120865           127366   \n",
       "1             127800           133940   \n",
       "2             134285           138166   \n",
       "3             139380           147611   \n",
       "4             148651           157866   \n",
       "\n",
       "                                            Yonaguni fileno  \n",
       "0  hata=nki m dama=nkí iti=ti (n tti) timunu sai ...      1  \n",
       "1  ndi=nki maru munu ni hamiru=ndí nta=ba unu ang...      2  \n",
       "2     da=gara tundiru munu dama=nki=ja hiranu=ti umi      3  \n",
       "3  aci-taburu=ndi taburu=du a=nga ata=ba umi udi ...      4  \n",
       "4  ja ubuda=nga nda=ja ba-nta=ja=ju ttui=ja h-i=d...      5  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subset by taking only those that don't end up being zero\n",
    "target_df = df[df['Yonaguni'].apply(lambda s: target_first(s))].reset_index(drop = True)\n",
    "\n",
    "print(len(target_df))\n",
    "\n",
    "target_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move files to target folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: os.mkdir(target_folder)\n",
    "except FileExistsError: pass\n",
    "\n",
    "for f in sorted(glob.glob(clipped_folder + '/*.wav')):\n",
    "    stem = Path(f).stem\n",
    "    if stem in target_df['fileno'].values:\n",
    "        os.rename(f, target_folder + '/' + stem + '.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Yonaguni transcription MFA friendly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_mfa = {\n",
    "    '1': '',\n",
    "    '=': '',\n",
    "    '-': '',\n",
    "    '\\(': '',\n",
    "    '\\)': '',\n",
    "    '\\?': '',\n",
    "    '\\.': ''\n",
    "}\n",
    "\n",
    "target_df['mfa'] = target_df['Yonaguni'].replace(to_mfa, regex = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to mark Japanese words so they won't be analyzed incorrectly as Yonaguni\n",
    "- Use regex to search for everything between < >\n",
    "- We will mark each Japanese word as beginning with \"q\" (MFA doesn't like symbols like < >) so that we can later filter them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_jp(sentence):\n",
    "    '''Takes sentence and looks for words between <> and adds q in front of each word'''\n",
    "    \n",
    "    # Find all instances of codeswitching\n",
    "    # [^(.+>.+<)]+ lets us get all instances if there is multiple codeswitching\n",
    "    matches = [m for m in re.finditer('<[^(.+>.+<)]+>', sentence)]\n",
    "    \n",
    "    # Then for parentheses\n",
    "    matches.extend([m for m in re.finditer('\\([^(.+\\).+\\()]+\\)', sentence)])\n",
    "    \n",
    "    if len(matches) == 0:\n",
    "        \n",
    "        return(sentence)\n",
    "    \n",
    "    else:\n",
    "    \n",
    "        # Initialize variable to keep track of how far into the word we are and our new sentence\n",
    "        i = 0\n",
    "        new_s = ''\n",
    "        \n",
    "        # Replace matches with q at the beginning of each word, so we know what the Japanese words are\n",
    "        for m in matches:\n",
    "            \n",
    "            # Shows what the code-switched segments are\n",
    "            print(m.group())\n",
    "\n",
    "            # Break up code-switched sentence into words\n",
    "            code_switch = m.group()[1:-1].split()\n",
    "\n",
    "            # Add q to beginning of every word\n",
    "            jp_marked = ' '.join(['q' + w for w in code_switch])\n",
    "            \n",
    "            # Conjoin to form new sentence\n",
    "            new_s += sentence[i:m.span()[0]] + jp_marked\n",
    "\n",
    "            # Update index\n",
    "            i = m.span()[1]\n",
    "\n",
    "        new_s += sentence[i:]\n",
    "        \n",
    "        return(new_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ttara nibanmeedakara>\n",
      "<genki>\n",
      "<kaza>\n",
      "<butsudan>\n",
      "<daitai>\n",
      "<niban>\n",
      "<iciban>\n",
      "<niban>\n",
      "<kaza>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       hatanki m damankí ititi n tti timunu sai sutaja\n",
       "1     ndinki maru munu ni hamirundí ntaba unu angami...\n",
       "2            dagara tundiru munu damankija hiranuti umi\n",
       "3     acitaburundi taburudu anga ataba umi udi anbir...\n",
       "4     ja ubudanga ndaja bantajaju ttuija hidu macind...\n",
       "5     utu nniti busa busa ndutasi nagudangadu naguda...\n",
       "6                        nagudanga magitaba nagudabagai\n",
       "7     damanki tui suta mata unu nuguru ttunu utudant...\n",
       "8     nagudanga tui sutaba timunu sai sutaba danki siti\n",
       "9     unindungadu nagudaja timunu ndu namakidu citi ...\n",
       "10    ntaba ijanga ja ija abutanga n o abuta abuta m...\n",
       "11    haatu maihuna angaminta atattumuti usi marumun...\n",
       "12    timunu mun n mujaninuti inamunundi umuiti na u...\n",
       "13    badija bungadu unu ki uni hadimiti damagara tu...\n",
       "14    abutanga tuiti kkurunu naganki icin iriti buta...\n",
       "15                   abutaja ibi matiti hananki naitima\n",
       "16    turitaba angamiti buuru dusi isiti abutaja ma ...\n",
       "17          qgenki minungara ndi maihuna maririjo nditi\n",
       "18    maisuruatingai unu angamintaja abuta abutandi ...\n",
       "19         abutaja ma kaisi kunutaba unu angamintangadu\n",
       "20    unu kinu kkuru naganki muti burungarandi nditi...\n",
       "21    abutandi umuiti qkazai utugarirunditi budu unu...\n",
       "22    ndi abutanki usutui usutuija abuta banu mabui ...\n",
       "23    ntaba unkaranki maa ubudanga banta unija naran...\n",
       "24                                   usi daa kkuiti nti\n",
       "25    qbutsudan tugundi ndunsuja qdaitai qniban dani...\n",
       "26    qniban danki u qkazaiti angamintaja buurusi us...\n",
       "27                dunannu tuguja nai burundi ndari buru\n",
       "Name: mfa, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check\n",
    "target_df['mfa'].apply(lambda s: mark_jp(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ttara nibanmeedakara>\n",
      "<genki>\n",
      "<kaza>\n",
      "<butsudan>\n",
      "<daitai>\n",
      "<niban>\n",
      "<iciban>\n",
      "<niban>\n",
      "<kaza>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Begin Time - msec</th>\n",
       "      <th>End Time - msec</th>\n",
       "      <th>Yonaguni</th>\n",
       "      <th>fileno</th>\n",
       "      <th>mfa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>120865</td>\n",
       "      <td>127366</td>\n",
       "      <td>hata=nki m dama=nkí iti=ti (n tti) timunu sai ...</td>\n",
       "      <td>1</td>\n",
       "      <td>hatanki m damankí ititi n tti timunu sai sutaja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>127800</td>\n",
       "      <td>133940</td>\n",
       "      <td>ndi=nki maru munu ni hamiru=ndí nta=ba unu ang...</td>\n",
       "      <td>2</td>\n",
       "      <td>ndinki maru munu ni hamirundí ntaba unu angami...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>134285</td>\n",
       "      <td>138166</td>\n",
       "      <td>da=gara tundiru munu dama=nki=ja hiranu=ti umi</td>\n",
       "      <td>3</td>\n",
       "      <td>dagara tundiru munu damankija hiranuti umi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>139380</td>\n",
       "      <td>147611</td>\n",
       "      <td>aci-taburu=ndi taburu=du a=nga ata=ba umi udi ...</td>\n",
       "      <td>4</td>\n",
       "      <td>acitaburundi taburudu anga ataba umi udi anbir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>148651</td>\n",
       "      <td>157866</td>\n",
       "      <td>ja ubuda=nga nda=ja ba-nta=ja=ju ttui=ja h-i=d...</td>\n",
       "      <td>5</td>\n",
       "      <td>ja ubudanga ndaja bantajaju ttuija hidu macind...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Begin Time - msec  End Time - msec  \\\n",
       "0             120865           127366   \n",
       "1             127800           133940   \n",
       "2             134285           138166   \n",
       "3             139380           147611   \n",
       "4             148651           157866   \n",
       "\n",
       "                                            Yonaguni fileno  \\\n",
       "0  hata=nki m dama=nkí iti=ti (n tti) timunu sai ...      1   \n",
       "1  ndi=nki maru munu ni hamiru=ndí nta=ba unu ang...      2   \n",
       "2     da=gara tundiru munu dama=nki=ja hiranu=ti umi      3   \n",
       "3  aci-taburu=ndi taburu=du a=nga ata=ba umi udi ...      4   \n",
       "4  ja ubuda=nga nda=ja ba-nta=ja=ju ttui=ja h-i=d...      5   \n",
       "\n",
       "                                                 mfa  \n",
       "0    hatanki m damankí ititi n tti timunu sai sutaja  \n",
       "1  ndinki maru munu ni hamirundí ntaba unu angami...  \n",
       "2         dagara tundiru munu damankija hiranuti umi  \n",
       "3  acitaburundi taburudu anga ataba umi udi anbir...  \n",
       "4  ja ubudanga ndaja bantajaju ttuija hidu macind...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_df['mfa'] = target_df['mfa'].apply(lambda s: mark_jp(s))\n",
    "\n",
    "target_df.to_csv(working_folder + '/' + working_folder + '-targets.csv', index = False)\n",
    "\n",
    "target_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use audiolabel to fill TextGrids with MFA transcription\n",
    "### First create TextGrids for all the files with the TextGridMaker Praat script\n",
    "- Use the TextGridMaker.praat script on the target folder\n",
    "- If the annotations are right after running this cell, replace the empty TextGrids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make directory for filled TextGrids if it does not exist\n",
    "try: os.mkdir(target_folder + '/filled')\n",
    "except FileExistsError: pass\n",
    "    \n",
    "for f in sorted(glob.glob(target_folder + '/*.TextGrid')):\n",
    "    stem = Path(f).stem\n",
    "    \n",
    "    # Get sentence from targets dataframe\n",
    "    sentence = target_df.loc[target_df['fileno'] == stem, 'mfa'].values[0]\n",
    "    \n",
    "    # Read TextGrid and replace text\n",
    "    with open(target_folder + '/' + stem + '.TextGrid', 'r') as f:\n",
    "        replace = re.sub('text = \".*\"', 'text = \"' + sentence + '\"', f.read())\n",
    "    \n",
    "    # Print to new file\n",
    "    with open(target_folder + '/filled/' + stem + '.TextGrid', 'w') as w:\n",
    "        w.write(replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make dictionary for MFA to phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfa_digraph = {\n",
    "    'si': 'SH IY1',\n",
    "    'sj': 'SH Y',\n",
    "    'zi': 'JH Y',\n",
    "    'zj': 'JH Y',\n",
    "}\n",
    "\n",
    "mfa_C = {\n",
    "    'q': '',\n",
    "    'ng': 'NG',\n",
    "    'nk': 'NG K',\n",
    "    'nm': 'M',\n",
    "    'nn': 'N',\n",
    "    'np': 'M P',\n",
    "    'nb': 'M B',\n",
    "    'j': 'Y',\n",
    "    'c': 'CH',\n",
    "    'kk': 'K',\n",
    "    'tt': 'T',\n",
    "    'h': 'HH'\n",
    "}\n",
    "\n",
    "mfa_VV = {\n",
    "    'aa': 'AA1',\n",
    "    'ee': 'EY1',\n",
    "    'ii': 'IY1',\n",
    "    'oo': 'OW1',\n",
    "    'uu': 'UW1'\n",
    "}\n",
    "\n",
    "mfa_V = {\n",
    "    'a': 'AA1',\n",
    "    'e': 'EY1',\n",
    "    'i': 'IY1',\n",
    "    'o': 'OW1',\n",
    "    'u': 'UW1'\n",
    "}\n",
    "\n",
    "# Add spaces on either end\n",
    "for d in [mfa_digraph, mfa_C, mfa_VV, mfa_V]:\n",
    "    for k, v in d.items():\n",
    "        d[k] = ' ' + v + ' '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for turning words to MFA pronunciation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_mfa(word):\n",
    "    '''Turns word to MFA pronunciation'''\n",
    "\n",
    "    for d in [mfa_digraph, mfa_C, mfa_VV, mfa_V]:\n",
    "        word = (multiple_replace(d, word))\n",
    "    \n",
    "    mfa = ''\n",
    "    \n",
    "    for l in word:\n",
    "        if l.islower():\n",
    "            mfa += ' ' + l.upper() + ' '\n",
    "        else:\n",
    "            mfa += l\n",
    "    \n",
    "    return(' '.join(mfa.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect the unique words that show up in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # IF WE NEED TO REREAD THE TARGET FILES\n",
    "\n",
    "# for folder in sorted(next(os.walk('.'))[1]):\n",
    "#     if folder != '.ipynb_checkpoints':\n",
    "#         target_df = pd.read_csv(folder + '/' + folder + '-targets.csv', keep_default_na = False)\n",
    "        \n",
    "#         words = []\n",
    "\n",
    "#         for s in target_df['mfa']:\n",
    "#             for w in s.split():\n",
    "#                 if w not in words: words.append(w)\n",
    "\n",
    "#         #print(sorted(words))\n",
    "\n",
    "#         with open(folder + '/dictionary.txt', 'w') as f:\n",
    "#         #with open(working_folder + '/dictionary.txt', 'w') as f:\n",
    "\n",
    "#             for w in sorted(words): \n",
    "#                 f.write(w + '  ' + to_mfa(w) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abuta', 'abutaja', 'abutajama', 'abutandi', 'abutanga', 'abutanki', 'acitaburundi', 'ai', 'anbi', 'anbiru', 'anga', 'angaminta', 'angamintaja', 'angamintangadu', 'angamiti', 'angamitintaja', 'angamitintanga', 'arungara', 'ataba', 'atara', 'atattumuti', 'atingai', 'atingaidu', 'badija', 'banta', 'bantaja', 'bantajaju', 'banu', 'budu', 'bungadu', 'buru', 'burundi', 'burungarandi', 'buruta', 'burutaatingai', 'burutasi', 'busa', 'butaru', 'butaruatingai', 'buuru', 'buurusi', 'cidi', 'citi', 'da', 'daa', 'dagara', 'damagara', 'damanki', 'damankija', 'damankí', 'dani', 'danidu', 'danki', 'din', 'dugui', 'dunannu', 'dusabinki', 'dusi', 'haatu', 'hadimiti', 'hai', 'hamirundí', 'hamirungarajo', 'hananki', 'hatanki', 'hataratidu', 'hataratiti', 'hidu', 'hiranuti', 'hjuru', 'ibi', 'icin', 'ija', 'ijanga', 'inamunundi', 'iriti', 'irumunujandi', 'isiti', 'ititi', 'ja', 'kaisi', 'ki', 'kidu', 'kinu', 'kirarirundi', 'kiti', 'kkuiti', 'kkuru', 'kkurunu', 'kunutaba', 'm', 'ma', 'maa', 'maasiku', 'mabui', 'macindi', 'magitaba', 'maihuna', 'maisuruatingai', 'maririjo', 'maru', 'marumunu', 'mata', 'matiti', 'minungara', 'mugitaba', 'mujaninuti', 'mun', 'munu', 'munudu', 'muti', 'n', 'na', 'naganki', 'naganu', 'nagudabagai', 'nagudaja', 'nagudanga', 'nagudangadu', 'nagudaqttara', 'nai', 'naigara', 'naitima', 'namaki', 'namakidu', 'naranungara', 'nataba', 'natitantin', 'ndaja', 'ndari', 'ndi', 'ndinki', 'ndiru', 'nditi', 'ndjangadu', 'ndu', 'ndunsuja', 'ndutasi', 'ni', 'nniti', 'ntaba', 'ntarumunugaradu', 'nti', 'nuguru', 'o', 'qbutsudan', 'qdaitai', 'qgenki', 'qiciban', 'qkazai', 'qkazaiti', 'qniban', 'qnibanmeedakara', 'sai', 'saiti', 'siti', 'sundo', 'suru', 'suta', 'sutaba', 'sutaja', 'taburudu', 'timunu', 'tti', 'ttuija', 'ttumuti', 'ttunja', 'ttunu', 'tuguja', 'tugundi', 'tui', 'tuiti', 'tundi', 'tundiru', 'turasi', 'turitaba', 'u', 'ubudanga', 'udi', 'udu', 'ugirja', 'uja', 'umi', 'umuindatana', 'umuiti', 'unga', 'uni', 'uniba', 'unija', 'unindungadu', 'unkaranki', 'unu', 'usi', 'usutui', 'usutuija', 'usutuini', 'utu', 'utudantaja', 'utugarirunditi', 'waiti', 'warijondi']\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "\n",
    "for s in target_df['mfa']:\n",
    "    for w in s.split():\n",
    "        if w not in words: words.append(w)\n",
    "            \n",
    "print(sorted(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(working_folder + '/dictionary.txt', 'w') as f:\n",
    "\n",
    "    for w in sorted(words): \n",
    "        f.write(w + '  ' + to_mfa(w) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "- The last prep work to be done for MFA alignment is isolating one channel (MFA doesn't deal well with two-channel files)\n",
    "- You can run the shell script extractchannel.sh in the command line to extract one channel (extract the channel where the speaker's voice is louder). The usage is in the script, but is repeated here\n",
    "\n",
    "        bash extractchannel.sh folder channel\n",
    "        \n",
    "        \n",
    "- Make sure the correct channel was extracted, then move the original target .wav files back to the clipped_folder. Then move these mono files to the target_folder\n",
    "- You can now run the MFA! See usage in mfa_usage.txt\n",
    "- After alignment, you can move the unaligned files into a folder called \"unaligned\"\n",
    "\n",
    "# Layout\n",
    "\n",
    "Below is a screenshot of how my folder layout looks after MFA realignment.\n",
    "\n",
    "- aligned : folder with target .wav files and MFA force aligned .TextGrids\n",
    "- clips : folder with original clipped files\n",
    "- unaligned : folder with target .TextGrids before forced alignment\n",
    "\n",
    "- dictionary.txt : dictionary for MFA forced alignment\n",
    "\n",
    "- [FILENAME].csv : .csv file containing metadata\n",
    "- [FILENAME].TextGrid : Praat .TextGrid extracted from ELAN .eaf file\n",
    "\n",
    "<img src='folderlayout.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import read_label from audiolabel to get TextGrid time info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiolabel import read_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get preceding and following words for each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contexts(word_data, phone_data):\n",
    "\n",
    "    '''Takes a word and phone dataframe and updates it for context'''\n",
    "    \n",
    "    # Word context\n",
    "    \n",
    "    prev_words = ['sentence_start']\n",
    "    prev_word_starts = ['sentence_start']\n",
    "    prev_word_ends = ['sentence_start']\n",
    "    \n",
    "    next_words = []\n",
    "    next_word_starts = []\n",
    "    next_word_ends = []\n",
    "    \n",
    "    # Times for previous\n",
    "\n",
    "    for i in range(0, len(word_data)):\n",
    "        \n",
    "        if i != 0:\n",
    "            prev_words.append(word_data.loc[i - 1, 'label'])\n",
    "            prev_word_starts.append(word_data.loc[i - 1, 't1'])\n",
    "            prev_word_ends.append(word_data.loc[i - 1, 't2'])\n",
    "            \n",
    "        if i != len(word_data) - 1:\n",
    "            next_words.append(word_data.loc[i + 1, 'label'])\n",
    "            next_word_starts.append(word_data.loc[i + 1, 't1'])\n",
    "            next_word_ends.append(word_data.loc[i + 1, 't2'])\n",
    "\n",
    "    next_words.append('sentence_end')\n",
    "    next_word_starts.append('sentence_end')\n",
    "    next_word_ends.append('sentence_end')\n",
    "\n",
    "    # Now add lists to datasets\n",
    "    word_data['prev_word'] = prev_words\n",
    "    word_data['prev_word_start'] = prev_word_starts\n",
    "    word_data['prev_word_end'] = prev_word_ends\n",
    "    \n",
    "    word_data['next_word'] = next_words\n",
    "    word_data['next_word_start'] = next_word_starts\n",
    "    word_data['next_word_end'] = next_word_ends\n",
    "    \n",
    "    # Get sentence and get rid of extra whitespaces\n",
    "    sentence = ' '.join((' '.join(word_data['label'].values)).split())\n",
    "    \n",
    "    word_data['sentence'] = sentence\n",
    "    \n",
    "    ###\n",
    "    \n",
    "    phone_data['word'] = phone_data.apply(lambda row : word_data[(word_data['t1'] <= row['t1']) & (word_data['t2'] >= row['t2'])]['label'].item(), axis = 1)\n",
    "    \n",
    "    phone_data['prev_word'] = phone_data.apply(lambda row : word_data[(word_data['t1'] <= row['t1']) & (word_data['t2'] >= row['t2'])]['prev_word'].item(), axis = 1)\n",
    "    phone_data['prev_word_start'] = phone_data.apply(lambda row : word_data[(word_data['t1'] <= row['t1']) & (word_data['t2'] >= row['t2'])]['prev_word_start'].item(), axis = 1)\n",
    "    phone_data['prev_word_end'] = phone_data.apply(lambda row : word_data[(word_data['t1'] <= row['t1']) & (word_data['t2'] >= row['t2'])]['prev_word_end'].item(), axis = 1)\n",
    "    \n",
    "    phone_data['next_word'] = phone_data.apply(lambda row : word_data[(word_data['t1'] <= row['t1']) & (word_data['t2'] >= row['t2'])]['next_word'].item(), axis = 1)\n",
    "    phone_data['next_word_start'] = phone_data.apply(lambda row : word_data[(word_data['t1'] <= row['t1']) & (word_data['t2'] >= row['t2'])]['next_word_start'].item(), axis = 1)\n",
    "    phone_data['next_word_end'] = phone_data.apply(lambda row : word_data[(word_data['t1'] <= row['t1']) & (word_data['t2'] >= row['t2'])]['next_word_end'].item(), axis = 1)\n",
    "    \n",
    "    phone_data['word_start'] = phone_data.apply(lambda row : word_data[(word_data['t1'] <= row['t1']) & (word_data['t2'] >= row['t2'])]['t1'].item(), axis = 1)\n",
    "    phone_data['word_end'] = phone_data.apply(lambda row : word_data[(word_data['t1'] <= row['t1']) & (word_data['t2'] >= row['t2'])]['t2'].item(), axis = 1)\n",
    "    \n",
    "    phone_data['sentence'] = sentence\n",
    "    \n",
    "    # Phone context\n",
    "\n",
    "    prev_phons = ['sentence_start']\n",
    "    prev_phon_starts = ['sentence_start']\n",
    "    prev_phon_ends = ['sentence_start']\n",
    "    \n",
    "    next_phons = []\n",
    "    next_phon_starts = []\n",
    "    next_phon_ends = []\n",
    "\n",
    "    for i in range(0, len(phone_data)):\n",
    "        \n",
    "        if i != 0:\n",
    "            prev_phons.append(phone_data.loc[i - 1, 'label'])\n",
    "            prev_phon_starts.append(phone_data.loc[i - 1, 't1'])\n",
    "            prev_phon_ends.append(phone_data.loc[i - 1, 't2'])\n",
    "            \n",
    "        if i != len(phone_data) - 1:\n",
    "            next_phons.append(phone_data.loc[i + 1, 'label'])\n",
    "            next_phon_starts.append(phone_data.loc[i + 1, 't1'])\n",
    "            next_phon_ends.append(phone_data.loc[i + 1, 't2'])\n",
    "            \n",
    "    next_phons.append('sentence_end')\n",
    "    next_phon_starts.append('sentence_end')\n",
    "    next_phon_ends.append('sentence_end')\n",
    "\n",
    "    phone_data['prev_phon'] = prev_phons\n",
    "    phone_data['prev_phon_start'] = prev_phon_starts\n",
    "    phone_data['prev_phon_end'] = prev_phon_ends\n",
    "    \n",
    "    phone_data['next_phon'] = next_phons\n",
    "    phone_data['next_phon_start'] = next_phon_starts\n",
    "    phone_data['next_phon_end'] = next_phon_ends\n",
    "    \n",
    "    return(word_data, phone_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df = pd.DataFrame()\n",
    "phone_df = pd.DataFrame()\n",
    "\n",
    "for f in glob.glob('*/aligned/*.TextGrid'):\n",
    "    \n",
    "    [sub_wdf, sub_pdf] = read_label(f, ftype = 'praat')\n",
    "    \n",
    "    [updated_wdf, updated_pdf] = get_contexts(sub_wdf, sub_pdf)\n",
    "    \n",
    "    word_df = pd.concat([word_df, updated_wdf])\n",
    "    phone_df = pd.concat([phone_df, updated_pdf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add phoneme and first syllable info\n",
    "\n",
    "# Define syllable\n",
    "def get_firstsyll(word):\n",
    "\n",
    "    vowels = ['a', 'i', 'u', 'e', 'o']\n",
    "\n",
    "    syll = ''\n",
    "\n",
    "    # Flags for onset, nucleus and coda\n",
    "    onset = 'incomplete'\n",
    "    nucleus = 'incomplete'\n",
    "\n",
    "    # If word is 2 for fewer letters long, just take it\n",
    "    if len(word) <= 2:\n",
    "\n",
    "        return(word)\n",
    "\n",
    "    # Keep moving i until we find a vowel\n",
    "    i = 0\n",
    "\n",
    "    # If first sound is vowel, no longer in onset\n",
    "    if word[0] in vowels:\n",
    "        onset = 'complete'\n",
    "\n",
    "    while onset != 'complete':\n",
    "\n",
    "        i += 1\n",
    "\n",
    "        syll += word[i-1]\n",
    "\n",
    "        if word[i] in vowels: \n",
    "\n",
    "            onset = 'complete'\n",
    "\n",
    "    # Look for end of word or coda\n",
    "    while nucleus != 'complete':\n",
    "\n",
    "        i += 1\n",
    "\n",
    "        syll += word[i-1]\n",
    "\n",
    "        if i == len(word):\n",
    "\n",
    "            return(syll)\n",
    "\n",
    "        if word[i] not in vowels:\n",
    "\n",
    "            nucleus = 'complete'\n",
    "\n",
    "    # If sound is n\n",
    "    if word[i] == 'n':\n",
    "\n",
    "        i += 1\n",
    "\n",
    "        # If whole word, then return\n",
    "        if i == len(word):\n",
    "\n",
    "            return(word)\n",
    "\n",
    "        # If next sound is consonant, add\n",
    "        if word[i] not in vowels:\n",
    "\n",
    "            syll += word[i-1]\n",
    "\n",
    "    return(syll)\n",
    "\n",
    "###\n",
    "\n",
    "def findonset(word):\n",
    "    \n",
    "    nuclei = ['a', 'e', 'i', 'o', 'u', 'w', 'j']\n",
    "    \n",
    "    onset = ''\n",
    "    \n",
    "    for phone in word:\n",
    "        \n",
    "        if phone not in nuclei:\n",
    "            \n",
    "            onset += phone\n",
    "        \n",
    "        else: return(onset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "phone_df['onset'] = phone_df['word'].apply(lambda w: findonset(w))\n",
    "phone_df['first_syll'] = phone_df['word'].apply(lambda w: get_firstsyll(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36169"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(phone_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t1</th>\n",
       "      <th>t2</th>\n",
       "      <th>label</th>\n",
       "      <th>fname</th>\n",
       "      <th>word</th>\n",
       "      <th>prev_word</th>\n",
       "      <th>prev_word_start</th>\n",
       "      <th>prev_word_end</th>\n",
       "      <th>next_word</th>\n",
       "      <th>next_word_start</th>\n",
       "      <th>...</th>\n",
       "      <th>word_end</th>\n",
       "      <th>sentence</th>\n",
       "      <th>prev_phon</th>\n",
       "      <th>prev_phon_start</th>\n",
       "      <th>prev_phon_end</th>\n",
       "      <th>next_phon</th>\n",
       "      <th>next_phon_start</th>\n",
       "      <th>next_phon_end</th>\n",
       "      <th>onset</th>\n",
       "      <th>first_syll</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.06</td>\n",
       "      <td>sil</td>\n",
       "      <td>YO0020c_die_see_seem-yoneshiro-20140709/aligne...</td>\n",
       "      <td></td>\n",
       "      <td>sentence_start</td>\n",
       "      <td>sentence_start</td>\n",
       "      <td>sentence_start</td>\n",
       "      <td>unu</td>\n",
       "      <td>0.06</td>\n",
       "      <td>...</td>\n",
       "      <td>0.06</td>\n",
       "      <td>unu ttuja nnidu bunsuja</td>\n",
       "      <td>sentence_start</td>\n",
       "      <td>sentence_start</td>\n",
       "      <td>sentence_start</td>\n",
       "      <td>UW1</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.18</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.18</td>\n",
       "      <td>UW1</td>\n",
       "      <td>YO0020c_die_see_seem-yoneshiro-20140709/aligne...</td>\n",
       "      <td>unu</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>ttuja</td>\n",
       "      <td>0.35</td>\n",
       "      <td>...</td>\n",
       "      <td>0.35</td>\n",
       "      <td>unu ttuja nnidu bunsuja</td>\n",
       "      <td>sil</td>\n",
       "      <td>0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>N</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.22</td>\n",
       "      <td></td>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.18</td>\n",
       "      <td>0.22</td>\n",
       "      <td>N</td>\n",
       "      <td>YO0020c_die_see_seem-yoneshiro-20140709/aligne...</td>\n",
       "      <td>unu</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>ttuja</td>\n",
       "      <td>0.35</td>\n",
       "      <td>...</td>\n",
       "      <td>0.35</td>\n",
       "      <td>unu ttuja nnidu bunsuja</td>\n",
       "      <td>UW1</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.18</td>\n",
       "      <td>UW1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.35</td>\n",
       "      <td></td>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.22</td>\n",
       "      <td>0.35</td>\n",
       "      <td>UW1</td>\n",
       "      <td>YO0020c_die_see_seem-yoneshiro-20140709/aligne...</td>\n",
       "      <td>unu</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>ttuja</td>\n",
       "      <td>0.35</td>\n",
       "      <td>...</td>\n",
       "      <td>0.35</td>\n",
       "      <td>unu ttuja nnidu bunsuja</td>\n",
       "      <td>N</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.22</td>\n",
       "      <td>T</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.45</td>\n",
       "      <td></td>\n",
       "      <td>u</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.45</td>\n",
       "      <td>T</td>\n",
       "      <td>YO0020c_die_see_seem-yoneshiro-20140709/aligne...</td>\n",
       "      <td>ttuja</td>\n",
       "      <td>unu</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.35</td>\n",
       "      <td></td>\n",
       "      <td>0.86</td>\n",
       "      <td>...</td>\n",
       "      <td>0.86</td>\n",
       "      <td>unu ttuja nnidu bunsuja</td>\n",
       "      <td>UW1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.35</td>\n",
       "      <td>UW1</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.49</td>\n",
       "      <td>tt</td>\n",
       "      <td>ttu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     t1    t2 label                                              fname   word  \\\n",
       "0  0.00  0.06   sil  YO0020c_die_see_seem-yoneshiro-20140709/aligne...          \n",
       "1  0.06  0.18   UW1  YO0020c_die_see_seem-yoneshiro-20140709/aligne...    unu   \n",
       "2  0.18  0.22     N  YO0020c_die_see_seem-yoneshiro-20140709/aligne...    unu   \n",
       "3  0.22  0.35   UW1  YO0020c_die_see_seem-yoneshiro-20140709/aligne...    unu   \n",
       "4  0.35  0.45     T  YO0020c_die_see_seem-yoneshiro-20140709/aligne...  ttuja   \n",
       "\n",
       "        prev_word prev_word_start   prev_word_end next_word next_word_start  \\\n",
       "0  sentence_start  sentence_start  sentence_start       unu            0.06   \n",
       "1                               0            0.06     ttuja            0.35   \n",
       "2                               0            0.06     ttuja            0.35   \n",
       "3                               0            0.06     ttuja            0.35   \n",
       "4             unu            0.06            0.35                      0.86   \n",
       "\n",
       "   ... word_end                 sentence       prev_phon prev_phon_start  \\\n",
       "0  ...     0.06  unu ttuja nnidu bunsuja  sentence_start  sentence_start   \n",
       "1  ...     0.35  unu ttuja nnidu bunsuja             sil               0   \n",
       "2  ...     0.35  unu ttuja nnidu bunsuja             UW1            0.06   \n",
       "3  ...     0.35  unu ttuja nnidu bunsuja               N            0.18   \n",
       "4  ...     0.86  unu ttuja nnidu bunsuja             UW1            0.22   \n",
       "\n",
       "    prev_phon_end next_phon next_phon_start next_phon_end onset first_syll  \n",
       "0  sentence_start       UW1            0.06          0.18  None             \n",
       "1            0.06         N            0.18          0.22                u  \n",
       "2            0.18       UW1            0.22          0.35                u  \n",
       "3            0.22         T            0.35          0.45                u  \n",
       "4            0.35       UW1            0.45          0.49    tt        ttu  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phone_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "k      4051\n",
       "m      3819\n",
       "       3549\n",
       "b      3200\n",
       "n      1976\n",
       "h      1741\n",
       "t      1679\n",
       "c      1227\n",
       "d      1188\n",
       "tt      983\n",
       "q       963\n",
       "s       778\n",
       "nn      652\n",
       "nd      414\n",
       "nt      317\n",
       "kk      309\n",
       "qk      309\n",
       "nm      297\n",
       "ts      233\n",
       "g       200\n",
       "ngg     169\n",
       "qt      161\n",
       "p       154\n",
       "qs      141\n",
       "nb      116\n",
       "ns      114\n",
       "qn       99\n",
       "ss       60\n",
       "nk       56\n",
       "qm       56\n",
       "qh       43\n",
       "qc       41\n",
       "qts      34\n",
       "qb       30\n",
       "ng       20\n",
       "qz       18\n",
       "qr       12\n",
       "qy       11\n",
       "mb       11\n",
       "qtt      10\n",
       "nts      10\n",
       "pp        9\n",
       "cc        3\n",
       "qnn       3\n",
       "qd        2\n",
       "Name: onset, dtype: int64"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phone_df['onset'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df.to_csv('words.csv', index = False)\n",
    "phone_df.to_csv('phones.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get target words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Raksit/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "/Users/Raksit/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "target_words = word_df[(word_df['label'].str.len() > 1) & (word_df['label'].str.contains('^[tkcpmn(ts)]'))]\n",
    "\n",
    "target_phones_all = phone_df[(phone_df['word'].str.len() > 1) & (phone_df['word'].str.contains('^[tkcpmn(ts)]')) & (phone_df['label'].isin(['T', 'K', 'CH', 'P', 'M', 'N'])) & (phone_df['word_start'] == phone_df['t1'])]\n",
    "\n",
    "target_phones_all['onset'] = target_phones_all['word'].apply(lambda w: findonset(w))\n",
    "target_phones_all['first_syll'] = target_phones_all['word'].apply(lambda w: get_firstsyll(w))\n",
    "\n",
    "# Reorder columns\n",
    "col_order = ['fname', 'onset', 'label', 't1', 't2',\n",
    "             'word', 'word_start', 'word_end',\n",
    "             'first_syll', 'sentence',\n",
    "             'prev_phon', 'prev_phon_start', 'prev_phon_end', 'next_phon', 'next_phon_start', 'next_phon_end', \n",
    "             'prev_word', 'prev_word_start', 'prev_word_end', 'next_word', 'next_word_start', 'next_word_end'\n",
    "             ]\n",
    "\n",
    "target_phones_all = target_phones_all[col_order]\n",
    "\n",
    "# Rename t1 and t2 to more useful names\n",
    "target_phones_all = target_phones_all.rename(columns = {'t1':'phon_start', 't2':'phon_end'})\n",
    "\n",
    "# Isolate phones that are preceded by vowels\n",
    "target_phones = target_phones_all[target_phones_all['prev_phon'].isin(['AA1', 'IY1', 'UW1', 'EY1', 'OW1', 'N'])].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "886\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>onset</th>\n",
       "      <th>label</th>\n",
       "      <th>phon_start</th>\n",
       "      <th>phon_end</th>\n",
       "      <th>word</th>\n",
       "      <th>word_start</th>\n",
       "      <th>word_end</th>\n",
       "      <th>first_syll</th>\n",
       "      <th>sentence</th>\n",
       "      <th>...</th>\n",
       "      <th>prev_phon_end</th>\n",
       "      <th>next_phon</th>\n",
       "      <th>next_phon_start</th>\n",
       "      <th>next_phon_end</th>\n",
       "      <th>prev_word</th>\n",
       "      <th>prev_word_start</th>\n",
       "      <th>prev_word_end</th>\n",
       "      <th>next_word</th>\n",
       "      <th>next_word_start</th>\n",
       "      <th>next_word_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>YO0020c_die_see_seem-yoneshiro-20140709/aligne...</td>\n",
       "      <td>tt</td>\n",
       "      <td>T</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.45</td>\n",
       "      <td>ttuja</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.86</td>\n",
       "      <td>ttu</td>\n",
       "      <td>unu ttuja nnidu bunsuja</td>\n",
       "      <td>...</td>\n",
       "      <td>0.35</td>\n",
       "      <td>UW1</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.49</td>\n",
       "      <td>unu</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.35</td>\n",
       "      <td></td>\n",
       "      <td>0.86</td>\n",
       "      <td>2.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>YO0020c_die_see_seem-yoneshiro-20140709/aligne...</td>\n",
       "      <td>nn</td>\n",
       "      <td>N</td>\n",
       "      <td>0.97</td>\n",
       "      <td>1.21</td>\n",
       "      <td>nniburu</td>\n",
       "      <td>0.97</td>\n",
       "      <td>1.62</td>\n",
       "      <td>nni</td>\n",
       "      <td>qterebidu nniburu</td>\n",
       "      <td>...</td>\n",
       "      <td>0.97</td>\n",
       "      <td>IY1</td>\n",
       "      <td>1.21</td>\n",
       "      <td>1.3</td>\n",
       "      <td>qterebidu</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.97</td>\n",
       "      <td></td>\n",
       "      <td>1.62</td>\n",
       "      <td>1.905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>YO0020c_die_see_seem-yoneshiro-20140709/aligne...</td>\n",
       "      <td>tt</td>\n",
       "      <td>T</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.76</td>\n",
       "      <td>ttutu</td>\n",
       "      <td>0.69</td>\n",
       "      <td>1.06</td>\n",
       "      <td>ttu</td>\n",
       "      <td>unu ttutu kunu ttutuja qttangadangara nni bunsuja</td>\n",
       "      <td>...</td>\n",
       "      <td>0.69</td>\n",
       "      <td>UW1</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.85</td>\n",
       "      <td>unu</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.69</td>\n",
       "      <td>kunu</td>\n",
       "      <td>1.06</td>\n",
       "      <td>1.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>YO0020c_die_see_seem-yoneshiro-20140709/aligne...</td>\n",
       "      <td>k</td>\n",
       "      <td>K</td>\n",
       "      <td>1.06</td>\n",
       "      <td>1.16</td>\n",
       "      <td>kunu</td>\n",
       "      <td>1.06</td>\n",
       "      <td>1.37</td>\n",
       "      <td>ku</td>\n",
       "      <td>unu ttutu kunu ttutuja qttangadangara nni bunsuja</td>\n",
       "      <td>...</td>\n",
       "      <td>1.06</td>\n",
       "      <td>UW1</td>\n",
       "      <td>1.16</td>\n",
       "      <td>1.23</td>\n",
       "      <td>ttutu</td>\n",
       "      <td>0.69</td>\n",
       "      <td>1.06</td>\n",
       "      <td>ttutuja</td>\n",
       "      <td>1.37</td>\n",
       "      <td>2.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>YO0020c_die_see_seem-yoneshiro-20140709/aligne...</td>\n",
       "      <td>tt</td>\n",
       "      <td>T</td>\n",
       "      <td>1.37</td>\n",
       "      <td>1.45</td>\n",
       "      <td>ttutuja</td>\n",
       "      <td>1.37</td>\n",
       "      <td>2.03</td>\n",
       "      <td>ttu</td>\n",
       "      <td>unu ttutu kunu ttutuja qttangadangara nni bunsuja</td>\n",
       "      <td>...</td>\n",
       "      <td>1.37</td>\n",
       "      <td>UW1</td>\n",
       "      <td>1.45</td>\n",
       "      <td>1.53</td>\n",
       "      <td>kunu</td>\n",
       "      <td>1.06</td>\n",
       "      <td>1.37</td>\n",
       "      <td></td>\n",
       "      <td>2.03</td>\n",
       "      <td>2.32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               fname onset label  phon_start  \\\n",
       "0  YO0020c_die_see_seem-yoneshiro-20140709/aligne...    tt     T        0.35   \n",
       "1  YO0020c_die_see_seem-yoneshiro-20140709/aligne...    nn     N        0.97   \n",
       "2  YO0020c_die_see_seem-yoneshiro-20140709/aligne...    tt     T        0.69   \n",
       "3  YO0020c_die_see_seem-yoneshiro-20140709/aligne...     k     K        1.06   \n",
       "4  YO0020c_die_see_seem-yoneshiro-20140709/aligne...    tt     T        1.37   \n",
       "\n",
       "   phon_end     word  word_start  word_end first_syll  \\\n",
       "0      0.45    ttuja        0.35      0.86        ttu   \n",
       "1      1.21  nniburu        0.97      1.62        nni   \n",
       "2      0.76    ttutu        0.69      1.06        ttu   \n",
       "3      1.16     kunu        1.06      1.37         ku   \n",
       "4      1.45  ttutuja        1.37      2.03        ttu   \n",
       "\n",
       "                                            sentence  ... prev_phon_end  \\\n",
       "0                            unu ttuja nnidu bunsuja  ...          0.35   \n",
       "1                                  qterebidu nniburu  ...          0.97   \n",
       "2  unu ttutu kunu ttutuja qttangadangara nni bunsuja  ...          0.69   \n",
       "3  unu ttutu kunu ttutuja qttangadangara nni bunsuja  ...          1.06   \n",
       "4  unu ttutu kunu ttutuja qttangadangara nni bunsuja  ...          1.37   \n",
       "\n",
       "  next_phon next_phon_start next_phon_end  prev_word prev_word_start  \\\n",
       "0       UW1            0.45          0.49        unu            0.06   \n",
       "1       IY1            1.21           1.3  qterebidu            0.38   \n",
       "2       UW1            0.76          0.85        unu            0.38   \n",
       "3       UW1            1.16          1.23      ttutu            0.69   \n",
       "4       UW1            1.45          1.53       kunu            1.06   \n",
       "\n",
       "  prev_word_end next_word next_word_start next_word_end  \n",
       "0          0.35                      0.86          2.55  \n",
       "1          0.97                      1.62         1.905  \n",
       "2          0.69      kunu            1.06          1.37  \n",
       "3          1.06   ttutuja            1.37          2.03  \n",
       "4          1.37                      2.03          2.32  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(target_phones))\n",
    "\n",
    "target_phones.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2863\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>onset</th>\n",
       "      <th>label</th>\n",
       "      <th>phon_start</th>\n",
       "      <th>phon_end</th>\n",
       "      <th>word</th>\n",
       "      <th>word_start</th>\n",
       "      <th>word_end</th>\n",
       "      <th>first_syll</th>\n",
       "      <th>sentence</th>\n",
       "      <th>...</th>\n",
       "      <th>prev_phon_end</th>\n",
       "      <th>next_phon</th>\n",
       "      <th>next_phon_start</th>\n",
       "      <th>next_phon_end</th>\n",
       "      <th>prev_word</th>\n",
       "      <th>prev_word_start</th>\n",
       "      <th>prev_word_end</th>\n",
       "      <th>next_word</th>\n",
       "      <th>next_word_start</th>\n",
       "      <th>next_word_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>YO0020c_die_see_seem-yoneshiro-20140709/aligne...</td>\n",
       "      <td>tt</td>\n",
       "      <td>T</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.45</td>\n",
       "      <td>ttuja</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.86</td>\n",
       "      <td>ttu</td>\n",
       "      <td>unu ttuja nnidu bunsuja</td>\n",
       "      <td>...</td>\n",
       "      <td>0.35</td>\n",
       "      <td>UW1</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.49</td>\n",
       "      <td>unu</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.35</td>\n",
       "      <td></td>\n",
       "      <td>0.86</td>\n",
       "      <td>2.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>YO0020c_die_see_seem-yoneshiro-20140709/aligne...</td>\n",
       "      <td>nn</td>\n",
       "      <td>N</td>\n",
       "      <td>2.55</td>\n",
       "      <td>2.79</td>\n",
       "      <td>nnidu</td>\n",
       "      <td>2.55</td>\n",
       "      <td>3.03</td>\n",
       "      <td>nni</td>\n",
       "      <td>unu ttuja nnidu bunsuja</td>\n",
       "      <td>...</td>\n",
       "      <td>2.55</td>\n",
       "      <td>IY1</td>\n",
       "      <td>2.79</td>\n",
       "      <td>2.9</td>\n",
       "      <td></td>\n",
       "      <td>0.86</td>\n",
       "      <td>2.55</td>\n",
       "      <td>bunsuja</td>\n",
       "      <td>3.03</td>\n",
       "      <td>3.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>YO0020c_die_see_seem-yoneshiro-20140709/aligne...</td>\n",
       "      <td>nn</td>\n",
       "      <td>N</td>\n",
       "      <td>0.97</td>\n",
       "      <td>1.21</td>\n",
       "      <td>nniburu</td>\n",
       "      <td>0.97</td>\n",
       "      <td>1.62</td>\n",
       "      <td>nni</td>\n",
       "      <td>qterebidu nniburu</td>\n",
       "      <td>...</td>\n",
       "      <td>0.97</td>\n",
       "      <td>IY1</td>\n",
       "      <td>1.21</td>\n",
       "      <td>1.3</td>\n",
       "      <td>qterebidu</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.97</td>\n",
       "      <td></td>\n",
       "      <td>1.62</td>\n",
       "      <td>1.905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>YO0020c_die_see_seem-yoneshiro-20140709/aligne...</td>\n",
       "      <td>tt</td>\n",
       "      <td>T</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.76</td>\n",
       "      <td>ttutu</td>\n",
       "      <td>0.69</td>\n",
       "      <td>1.06</td>\n",
       "      <td>ttu</td>\n",
       "      <td>unu ttutu kunu ttutuja qttangadangara nni bunsuja</td>\n",
       "      <td>...</td>\n",
       "      <td>0.69</td>\n",
       "      <td>UW1</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.85</td>\n",
       "      <td>unu</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.69</td>\n",
       "      <td>kunu</td>\n",
       "      <td>1.06</td>\n",
       "      <td>1.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>YO0020c_die_see_seem-yoneshiro-20140709/aligne...</td>\n",
       "      <td>k</td>\n",
       "      <td>K</td>\n",
       "      <td>1.06</td>\n",
       "      <td>1.16</td>\n",
       "      <td>kunu</td>\n",
       "      <td>1.06</td>\n",
       "      <td>1.37</td>\n",
       "      <td>ku</td>\n",
       "      <td>unu ttutu kunu ttutuja qttangadangara nni bunsuja</td>\n",
       "      <td>...</td>\n",
       "      <td>1.06</td>\n",
       "      <td>UW1</td>\n",
       "      <td>1.16</td>\n",
       "      <td>1.23</td>\n",
       "      <td>ttutu</td>\n",
       "      <td>0.69</td>\n",
       "      <td>1.06</td>\n",
       "      <td>ttutuja</td>\n",
       "      <td>1.37</td>\n",
       "      <td>2.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               fname onset label  phon_start  \\\n",
       "4  YO0020c_die_see_seem-yoneshiro-20140709/aligne...    tt     T        0.35   \n",
       "9  YO0020c_die_see_seem-yoneshiro-20140709/aligne...    nn     N        2.55   \n",
       "9  YO0020c_die_see_seem-yoneshiro-20140709/aligne...    nn     N        0.97   \n",
       "4  YO0020c_die_see_seem-yoneshiro-20140709/aligne...    tt     T        0.69   \n",
       "8  YO0020c_die_see_seem-yoneshiro-20140709/aligne...     k     K        1.06   \n",
       "\n",
       "   phon_end     word  word_start  word_end first_syll  \\\n",
       "4      0.45    ttuja        0.35      0.86        ttu   \n",
       "9      2.79    nnidu        2.55      3.03        nni   \n",
       "9      1.21  nniburu        0.97      1.62        nni   \n",
       "4      0.76    ttutu        0.69      1.06        ttu   \n",
       "8      1.16     kunu        1.06      1.37         ku   \n",
       "\n",
       "                                            sentence  ... prev_phon_end  \\\n",
       "4                            unu ttuja nnidu bunsuja  ...          0.35   \n",
       "9                            unu ttuja nnidu bunsuja  ...          2.55   \n",
       "9                                  qterebidu nniburu  ...          0.97   \n",
       "4  unu ttutu kunu ttutuja qttangadangara nni bunsuja  ...          0.69   \n",
       "8  unu ttutu kunu ttutuja qttangadangara nni bunsuja  ...          1.06   \n",
       "\n",
       "  next_phon next_phon_start next_phon_end  prev_word prev_word_start  \\\n",
       "4       UW1            0.45          0.49        unu            0.06   \n",
       "9       IY1            2.79           2.9                       0.86   \n",
       "9       IY1            1.21           1.3  qterebidu            0.38   \n",
       "4       UW1            0.76          0.85        unu            0.38   \n",
       "8       UW1            1.16          1.23      ttutu            0.69   \n",
       "\n",
       "  prev_word_end next_word next_word_start next_word_end  \n",
       "4          0.35                      0.86          2.55  \n",
       "9          2.55   bunsuja            3.03          3.86  \n",
       "9          0.97                      1.62         1.905  \n",
       "4          0.69      kunu            1.06          1.37  \n",
       "8          1.06   ttutuja            1.37          2.03  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(target_phones_all))\n",
    "\n",
    "target_phones_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_phones.to_csv('target_phones.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_phones_all.to_csv('target_phones_all.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "m     277\n",
       "k     188\n",
       "n     122\n",
       "t      97\n",
       "tt     56\n",
       "nn     43\n",
       "nd     20\n",
       "c      19\n",
       "nt     16\n",
       "nm     16\n",
       "ns     10\n",
       "kk      8\n",
       "ts      5\n",
       "nb      4\n",
       "p       3\n",
       "mb      1\n",
       "pp      1\n",
       "Name: onset, dtype: int64"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_phones['onset'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "k      653\n",
       "m      597\n",
       "n      345\n",
       "t      288\n",
       "c      254\n",
       "tt     241\n",
       "nn     142\n",
       "nd      81\n",
       "nm      62\n",
       "kk      55\n",
       "nt      45\n",
       "ts      35\n",
       "nb      22\n",
       "p       22\n",
       "ns      17\n",
       "mb       1\n",
       "cc       1\n",
       "pp       1\n",
       "nts      1\n",
       "Name: onset, dtype: int64"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_phones_all['onset'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_all.groupby(['onset'])['word'].value_counts().reset_index(name = 'count').to_csv('word_count_all.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_phones.groupby(['onset'])['word'].value_counts().reset_index(name = 'count').to_csv('word_count.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_phones.groupby(['onset'])['first_syll'].value_counts().reset_index(name = 'count').to_csv('first_syll_count.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets_all.groupby(['onset'])['first_syll'].value_counts().reset_index(name = 'count').to_csv('first_syll_count_all.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = loadmat('YoneshiroTable.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s0', 's1', 's2', 'arr']"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdata = mat['None']\n",
    "mdtype = mdata.dtype\n",
    "\n",
    "mdtype\n",
    "\n",
    "[n for n in mdtype.names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MatlabOpaque([b'tbl'], dtype=object)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mdata['s0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# * SciPy reads in structures as structured NumPy arrays of dtype object\n",
    "# * The size of the array is the size of the structure array, not the number\n",
    "#   elements in any particular field. The shape defaults to 2-dimensional.\n",
    "# * For convenience make a dictionary of the data using the names from dtypes\n",
    "# * Since the structure has only one element, but is 2-D, index it at [0, 0]\n",
    "ndata = {n: mdata[n][0] for n in mdtype.names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruct the columns of the data table from just the time series\n",
    "# Use the number of intervals to test if a field is a column or metadata\n",
    "columns = [n for n, v in ndata.items()] #if v.size == ndata['numIntervals']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "zero-dimensional arrays cannot be concatenated",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-221-948d08d96390>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# now make a data frame, setting the time stamps as the index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m df = pd.DataFrame(np.concatenate([ndata[c] for c in columns], axis=1),\n\u001b[0m\u001b[1;32m      3\u001b[0m                   \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mts\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mndata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'timestamps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                   columns=columns)\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: zero-dimensional arrays cannot be concatenated"
     ]
    }
   ],
   "source": [
    "# now make a data frame, setting the time stamps as the index\n",
    "df = pd.DataFrame(np.concatenate([ndata[c] for c in columns], axis=1),\n",
    "                  index=[datetime(*ts) for ts in ndata['timestamps']],\n",
    "                  columns=columns)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
