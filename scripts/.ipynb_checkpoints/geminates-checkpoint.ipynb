{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Yonaguni transcriptions for MFA forced aligner\n",
    "- This script processes Yonaguni transcriptions from ELAN in the following ways:\n",
    "    \n",
    "    - Modifies the ELAN transcriptions .csv file into a dataframe with start and end times and MFA transcriptions for each annotation\n",
    "\n",
    "    - Moves the transcriptions that have the target phonemes to a folder for alignment\n",
    "\n",
    "    - Fills in the textgrids automatically with MFA friendly transcriptions\n",
    "\n",
    "    - Builds a dictionary mapping the transcriptions the MFA can process\n",
    "     \n",
    "- Before starting this script, you should\n",
    "\n",
    "    1) Have your ELAN file fully annotated. If you have empty annotations segmented out, fill them in with \"xxx\" for now\n",
    "    \n",
    "    2) In ELAN, export your .eaf file as a Praat .TextGrid and as a .csv file (Tab Delimited is the option under File)\n",
    "    \n",
    "    3) Load the whole .wav file as a LongSound File and the .TextGrid file you just exported into Praat \n",
    "    \n",
    "    4) Make a clips folder and use the save_intervals_to_wav_sound_files.praat script (skip empty annotations) to cut the .wav file into the annotated chunks\n",
    "    \n",
    "    5) This should produce a folder with the clipped .wav files numbered 1 through however many annotations you have + a .txt file with all the annotations\n",
    "\n",
    "- Now name the following folders\n",
    "\n",
    "    - working_folder : the overall folder you are working with\n",
    "    - clipped_folder : folder where you have the original clipped files from the script\n",
    "    - target_folder : folder for the files with the target segments to be moved to\n",
    "    \n",
    "## You can scroll down to the bottom to see a screenshot of what the folders could look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHOOSE THE FILE\n",
    "file = 'YO0004_yoneshiro_imperative_conjunction_20130220'\n",
    "\n",
    "working_folder = '../files/' + file\n",
    "clipped_folder = working_folder + '/clips'\n",
    "target_folder = working_folder + '/targets-stereo'\n",
    "aligned_folder = working_folder + '/aligned'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import nested_scopes\n",
    "\n",
    "import pandas as pd\n",
    "import glob, os\n",
    "import numpy as np\n",
    "import audiolabel as al\n",
    "import re\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_replace(adict, text):\n",
    "    '''Takes a dictionary and a string and replaces keys with values'''\n",
    "    # Create a regular expression from all of the dictionary keys\n",
    "    regex = re.compile(\"|\".join(map(re.escape, adict.keys(  ))))\n",
    "    \n",
    "    # For each match, look up the corresponding value in the dictionary\n",
    "    return regex.sub(lambda match: adict[match.group(0)], text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in file that we are managing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Begin Time - msec</th>\n",
       "      <th>End Time - msec</th>\n",
       "      <th>Yonaguni</th>\n",
       "      <th>fileno</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21047</td>\n",
       "      <td>21447</td>\n",
       "      <td>k-u-n</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60661</td>\n",
       "      <td>61696</td>\n",
       "      <td>agai=du bur-u</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68433</td>\n",
       "      <td>68943</td>\n",
       "      <td>da</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>83276</td>\n",
       "      <td>83851</td>\n",
       "      <td>hir-u-n</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>98664</td>\n",
       "      <td>99986</td>\n",
       "      <td>tabi=nki hir-u-n</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Begin Time - msec  End Time - msec          Yonaguni fileno\n",
       "0              21047            21447             k-u-n      1\n",
       "1              60661            61696     agai=du bur-u      2\n",
       "2              68433            68943                da      3\n",
       "3              83276            83851           hir-u-n      4\n",
       "4              98664            99986  tabi=nki hir-u-n      5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = working_folder + '/' + file + '.csv'\n",
    "\n",
    "df = pd.read_csv(f, keep_default_na = False)\n",
    "\n",
    "# Drop unnamed columns\n",
    "df = df.drop([c for c in df.columns if 'Unnamed' in c], axis = 1)\n",
    "\n",
    "df['fileno'] = df.index + 1\n",
    "df['fileno'] = df['fileno'].apply(str)\n",
    "\n",
    "# Add column for number corresponding to file number\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'YS_Transcription-txt-rys' in df.columns:\n",
    "    df = df.rename(columns = {'YS_Transcription-txt-rys':'Yonaguni'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only want files that begin with these phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ['p', 't', 'k', 'c', 'n', 'm', 'ts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_first(sentence):\n",
    "    '''Breaks up the sentence by spaces and looks at the first letter for each word'''\n",
    "    # Get rid of Japanese code switching or uncertainties (parentheses)\n",
    "    sentence_nojp = re.sub('<[^(.+>.+<)]+>', '', sentence)\n",
    "    sentence_nojp = re.sub('\\([^(.+\\).+\\()]+\\)', '', sentence_nojp)\n",
    "    \n",
    "    # Look at the first letter in each word. Ignore n\n",
    "    first_letters = [w[0] for w in sentence_nojp.split() if len(w) > 1]\n",
    "    # And first 2, in case ts\n",
    "    first_digraph = [w[0:2] for w in sentence_nojp.split() if len(w) > 1]\n",
    "    \n",
    "    # Only take those that are overlapped\n",
    "    inter = set.intersection(set(first_letters), set(targets))\n",
    "    \n",
    "    return(len(inter) > 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply function to subset dataframe to just those tokens that contain our targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "223\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Begin Time - msec</th>\n",
       "      <th>End Time - msec</th>\n",
       "      <th>Yonaguni</th>\n",
       "      <th>fileno</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21047</td>\n",
       "      <td>21447</td>\n",
       "      <td>k-u-n</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98664</td>\n",
       "      <td>99986</td>\n",
       "      <td>tabi=nki hir-u-n</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>111279</td>\n",
       "      <td>112254</td>\n",
       "      <td>kkar-ir-u-n</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>123518</td>\n",
       "      <td>124213</td>\n",
       "      <td>kka-ni-nu-n</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>191060</td>\n",
       "      <td>191960</td>\n",
       "      <td>kum-i bur-a-nu</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Begin Time - msec  End Time - msec          Yonaguni fileno\n",
       "0              21047            21447             k-u-n      1\n",
       "1              98664            99986  tabi=nki hir-u-n      5\n",
       "2             111279           112254       kkar-ir-u-n      6\n",
       "3             123518           124213       kka-ni-nu-n      7\n",
       "4             191060           191960    kum-i bur-a-nu      9"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subset by taking only those that don't end up being zero\n",
    "target_df = df[df['Yonaguni'].apply(lambda s: target_first(s))].reset_index(drop = True)\n",
    "\n",
    "print(len(target_df))\n",
    "\n",
    "target_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Move files to target folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: os.mkdir(target_folder)\n",
    "except FileExistsError: pass\n",
    "\n",
    "for f in sorted(glob.glob(clipped_folder + '/*.wav')):\n",
    "    stem = Path(f).stem\n",
    "    if stem in target_df['fileno'].values:\n",
    "        os.rename(f, target_folder + '/' + stem + '.wav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Yonaguni transcription MFA friendly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_mfa = {\n",
    "    '1': '',\n",
    "    '=': '',\n",
    "    '-': '',\n",
    "    '\\(': '',\n",
    "    '\\)': '',\n",
    "    '\\?': '',\n",
    "    '\\.': ''\n",
    "}\n",
    "\n",
    "target_df['mfa'] = target_df['Yonaguni'].replace(to_mfa, regex = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to mark Japanese words so they won't be analyzed incorrectly as Yonaguni\n",
    "- Use regex to search for everything between < >\n",
    "- We will mark each Japanese word as beginning with \"q\" (MFA doesn't like symbols like < >) so that we can later filter them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_jp(sentence):\n",
    "    '''Takes sentence and looks for words between <> and adds q in front of each word'''\n",
    "    \n",
    "    # Find all instances of codeswitching\n",
    "    # [^(.+>.+<)]+ lets us get all instances if there is multiple codeswitching\n",
    "    matches = [m for m in re.finditer('<[^(.+>.+<)]+>', sentence)]\n",
    "    \n",
    "    # Then for parentheses\n",
    "    matches.extend([m for m in re.finditer('\\([^(.+\\).+\\()]+\\)', sentence)])\n",
    "    \n",
    "    if len(matches) == 0:\n",
    "        \n",
    "        return(sentence)\n",
    "    \n",
    "    else:\n",
    "    \n",
    "        # Initialize variable to keep track of how far into the word we are and our new sentence\n",
    "        i = 0\n",
    "        new_s = ''\n",
    "        \n",
    "        # Replace matches with q at the beginning of each word, so we know what the Japanese words are\n",
    "        for m in matches:\n",
    "            \n",
    "            # Shows what the code-switched segments are\n",
    "            print(m.group())\n",
    "\n",
    "            # Break up code-switched sentence into words\n",
    "            code_switch = m.group()[1:-1].split()\n",
    "\n",
    "            # Add q to beginning of every word\n",
    "            jp_marked = ' '.join(['q' + w for w in code_switch])\n",
    "            \n",
    "            # Conjoin to form new sentence\n",
    "            new_s += sentence[i:m.span()[0]] + jp_marked\n",
    "\n",
    "            # Update index\n",
    "            i = m.span()[1]\n",
    "\n",
    "        new_s += sentence[i:]\n",
    "        \n",
    "        return(new_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<terebi>\n",
      "<terebi>\n",
      "<terebi>\n",
      "<terebi>\n",
      "<jasai>\n",
      "<urawa>\n",
      "<tokajo>\n",
      "<jeega>\n",
      "<jeega>\n",
      "<jeega>\n",
      "<m sono tokimo ieru>\n",
      "<zjuuichizi>\n",
      "<zjuuichizi>\n",
      "<zjuuichizi>\n",
      "<aujo>\n",
      "<toga ittandessjo>\n",
      "<to iukara>\n",
      "<katazuke>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                           kun\n",
       "1                 tabinki hirun\n",
       "2                      kkarirun\n",
       "3                      kkaninun\n",
       "4                   kumi buranu\n",
       "                 ...           \n",
       "218         tagabiti maaminutan\n",
       "219                ki hindagijo\n",
       "220    maaminubiti tagadu ataru\n",
       "221                tagadu ataru\n",
       "222                tagadu ataru\n",
       "Name: mfa, Length: 223, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check\n",
    "target_df['mfa'].apply(lambda s: mark_jp(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<terebi>\n",
      "<terebi>\n",
      "<terebi>\n",
      "<terebi>\n",
      "<jasai>\n",
      "<urawa>\n",
      "<tokajo>\n",
      "<jeega>\n",
      "<jeega>\n",
      "<jeega>\n",
      "<m sono tokimo ieru>\n",
      "<zjuuichizi>\n",
      "<zjuuichizi>\n",
      "<zjuuichizi>\n",
      "<aujo>\n",
      "<toga ittandessjo>\n",
      "<to iukara>\n",
      "<katazuke>\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Begin Time - msec</th>\n",
       "      <th>End Time - msec</th>\n",
       "      <th>Yonaguni</th>\n",
       "      <th>fileno</th>\n",
       "      <th>mfa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21047</td>\n",
       "      <td>21447</td>\n",
       "      <td>k-u-n</td>\n",
       "      <td>1</td>\n",
       "      <td>kun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98664</td>\n",
       "      <td>99986</td>\n",
       "      <td>tabi=nki hir-u-n</td>\n",
       "      <td>5</td>\n",
       "      <td>tabinki hirun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>111279</td>\n",
       "      <td>112254</td>\n",
       "      <td>kkar-ir-u-n</td>\n",
       "      <td>6</td>\n",
       "      <td>kkarirun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>123518</td>\n",
       "      <td>124213</td>\n",
       "      <td>kka-ni-nu-n</td>\n",
       "      <td>7</td>\n",
       "      <td>kkaninun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>191060</td>\n",
       "      <td>191960</td>\n",
       "      <td>kum-i bur-a-nu</td>\n",
       "      <td>9</td>\n",
       "      <td>kumi buranu</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Begin Time - msec  End Time - msec          Yonaguni fileno            mfa\n",
       "0              21047            21447             k-u-n      1            kun\n",
       "1              98664            99986  tabi=nki hir-u-n      5  tabinki hirun\n",
       "2             111279           112254       kkar-ir-u-n      6       kkarirun\n",
       "3             123518           124213       kka-ni-nu-n      7       kkaninun\n",
       "4             191060           191960    kum-i bur-a-nu      9    kumi buranu"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_df['mfa'] = target_df['mfa'].apply(lambda s: mark_jp(s))\n",
    "\n",
    "target_df.to_csv(working_folder + '/' + file + '-targets.csv', index = False)\n",
    "\n",
    "target_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use audiolabel to fill TextGrids with MFA transcription\n",
    "### First create TextGrids for all the files with the TextGridMaker Praat script\n",
    "- Use the TextGridMaker.praat script on the target folder\n",
    "- If the annotations are right after running this cell, replace the empty TextGrids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make directory for filled TextGrids if it does not exist\n",
    "try: os.mkdir(aligned_folder + '/filled')\n",
    "except FileExistsError: pass\n",
    "    \n",
    "for f in sorted(glob.glob(aligned_folder + '/*.TextGrid')):\n",
    "    stem = Path(f).stem\n",
    "    \n",
    "    # Get sentence from targets dataframe\n",
    "    sentence = target_df.loc[target_df['fileno'] == stem, 'mfa'].values[0]\n",
    "    \n",
    "    # Read TextGrid and replace text\n",
    "    with open(aligned_folder + '/' + stem + '.TextGrid', 'r') as f:\n",
    "        replace = re.sub('text = \".*\"', 'text = \"' + sentence + '\"', f.read())\n",
    "    \n",
    "    # Print to new file\n",
    "    with open(aligned_folder + '/filled/' + stem + '.TextGrid', 'w') as w:\n",
    "        w.write(replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make dictionary for MFA to phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mfa_digraph = {\n",
    "    'si': 'SH IY1',\n",
    "    'sj': 'SH Y',\n",
    "    'zi': 'JH Y',\n",
    "    'zj': 'JH Y',\n",
    "}\n",
    "\n",
    "mfa_C = {\n",
    "    'q': '',\n",
    "    'ng': 'NG',\n",
    "    'nk': 'NG K',\n",
    "    'nm': 'M',\n",
    "    'nn': 'N',\n",
    "    'np': 'M P',\n",
    "    'nb': 'M B',\n",
    "    'j': 'Y',\n",
    "    'c': 'CH',\n",
    "    'kk': 'K',\n",
    "    'tt': 'T',\n",
    "    'h': 'HH'\n",
    "}\n",
    "\n",
    "mfa_VV = {\n",
    "    'aa': 'AA1',\n",
    "    'ee': 'EY1',\n",
    "    'ii': 'IY1',\n",
    "    'oo': 'OW1',\n",
    "    'uu': 'UW1'\n",
    "}\n",
    "\n",
    "mfa_V = {\n",
    "    'a': 'AA1',\n",
    "    'e': 'EY1',\n",
    "    'i': 'IY1',\n",
    "    'o': 'OW1',\n",
    "    'u': 'UW1'\n",
    "}\n",
    "\n",
    "# Add spaces on either end\n",
    "for d in [mfa_digraph, mfa_C, mfa_VV, mfa_V]:\n",
    "    for k, v in d.items():\n",
    "        d[k] = ' ' + v + ' '"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for turning words to MFA pronunciation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_mfa(word):\n",
    "    '''Turns word to MFA pronunciation'''\n",
    "\n",
    "    for d in [mfa_digraph, mfa_C, mfa_VV, mfa_V]:\n",
    "        word = (multiple_replace(d, word))\n",
    "    \n",
    "    mfa = ''\n",
    "    \n",
    "    for l in word:\n",
    "        if l.islower():\n",
    "            mfa += ' ' + l.upper() + ' '\n",
    "        else:\n",
    "            mfa += l\n",
    "    \n",
    "    return(' '.join(mfa.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect the unique words that show up in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # IF WE NEED TO REREAD THE TARGET FILES\n",
    "\n",
    "# for folder in sorted(next(os.walk('.'))[1]):\n",
    "#     if folder != '.ipynb_checkpoints':\n",
    "#         target_df = pd.read_csv(folder + '/' + folder + '-targets.csv', keep_default_na = False)\n",
    "        \n",
    "#         words = []\n",
    "\n",
    "#         for s in target_df['mfa']:\n",
    "#             for w in s.split():\n",
    "#                 if w not in words: words.append(w)\n",
    "\n",
    "#         #print(sorted(words))\n",
    "\n",
    "#         with open(folder + '/dictionary.txt', 'w') as f:\n",
    "#         #with open(working_folder + '/dictionary.txt', 'w') as f:\n",
    "\n",
    "#             for w in sorted(words): \n",
    "#                 f.write(w + '  ' + to_mfa(w) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abatidu', 'abatinnajo', 'abatiti', 'abjaminunggara', 'abunki', 'abunkidu', 'abunkija', 'agami', 'aigundo', 'aitando', 'aitarasi', 'aitarasidu', 'amidu', 'amiri', 'amirijo', 'amiti', 'amitidu', 'angami', 'angamidu', 'ansuja', 'arai', 'araiti', 'araitigara', 'arando', 'aranuna', 'arataru', 'aru', 'arunggara', 'atadu', 'ataru', 'atsadarunggara', 'atsaru', 'bagaruta', 'barasadu', 'bata', 'buna', 'bunga', 'bungara', 'buranu', 'buru', 'burungara', 'burunggara', 'butaru', 'butarugai', 'ci', 'cikaran', 'cira', 'ciradu', 'dagara', 'danki', 'dannaidu', 'din', 'dinadu', 'dinnga', 'ducitu', 'dutuni', 'gakunki', 'habagin', 'habain', 'habainja', 'haitigara', 'hajagu', 'hangara', 'hanuti', 'hataba', 'hi', 'hima', 'hindagijo', 'hinnajo', 'hiri', 'hirijo', 'hirudo', 'hirun', 'hirundo', 'hjundo', 'huganunga', 'hui', 'i', 'ibitati', 'isi', 'kaci', 'kacidu', 'kacija', 'kadin', 'kaidu', 'kaindi', 'kaisi', 'kanundo', 'ki', 'kidu', 'kinnajo', 'kiranunga', 'kirunga', 'kitando', 'kitanga', 'kjando', 'kkaninun', 'kkaninutangara', 'kkaninutanggara', 'kkarirun', 'kumi', 'kun', 'kunduguri', 'kunduguridu', 'kunduguriru', 'kunnajo', 'kunninu', 'kutuba', 'kutuja', 'ma', 'maa', 'maaminubiti', 'maaminutan', 'maasiku', 'maci', 'madi', 'mamuni', 'matidu', 'matijo', 'min', 'minaganki', 'minaganu', 'mingadu', 'mingaduja', 'mingaduqtokajo', 'minuga', 'minun', 'minunga', 'minungara', 'misijani', 'misijanki', 'mja', 'mucikasadu', 'muti', 'nai', 'naija', 'nda', 'ndi', 'ndini', 'ndunnajo', 'nindidu', 'nindijo', 'nindiru', 'nindudo', 'nnanun', 'nnari', 'nnaridu', 'nnijo', 'nnindagijo', 'nnu', 'nnudo', 'nnuja', 'nnun', 'nnundo', 'nnunnajo', 'ntidu', 'nuba', 'nudindagijo', 'nudiru', 'numitaba', 'numiti', 'nundiba', 'qaujo', 'qieru', 'qittandessjo', 'qiukara', 'qjasai', 'qjeegadu', 'qkatazukeranunga', 'qm', 'qsono', 'qterebi', 'qtokimo', 'qurawa', 'qzjuuichizibagin', 'qzjuuichizikuta', 'sagi', 'sibabiti', 'su', 'sui', 'susabiti', 'susadaru', 'suzi', 'tabinki', 'tagabiti', 'tagadu', 'tatijo', 'tin', 'tsa', 'tsadu', 'tsu', 'tsundi', 'tsungara', 'tta', 'ttuja', 'ttumuci', 'ttumunu', 'ttumuti', 'tubaiti', 'tubaru', 'tubarundidu', 'tundi', 'tunditidu', 'tundundo', 'turijo', 'turiqto', 'turuqtoga', 'tutarudo', 'ugitaba', 'ugitaja', 'ujasi', 'umanki', 'umutsaanggara', 'umutsadu', 'umutsanggara', 'unninu', 'unu', 'waini', 'wanna', 'wannajo', 'waranuna', 'wari', 'warijo', 'warunga']\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "\n",
    "for s in target_df['mfa']:\n",
    "    for w in s.split():\n",
    "        if w not in words: words.append(w)\n",
    "            \n",
    "print(sorted(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(working_folder + '/dictionary.txt', 'w') as f:\n",
    "\n",
    "    for w in sorted(words): \n",
    "        f.write(w + '  ' + to_mfa(w) + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "- The last prep work to be done for MFA alignment is isolating one channel (MFA doesn't deal well with two-channel files)\n",
    "- You can run the shell script extractchannel.sh in the command line to extract one channel (extract the channel where the speaker's voice is louder). The usage is in the script, but is repeated here\n",
    "\n",
    "        bash extractchannel.sh folder channel\n",
    "        \n",
    "        \n",
    "- Make sure the correct channel was extracted, then move the original target .wav files back to the clipped_folder. Then move these mono files to the target_folder\n",
    "- You can now run the MFA! See usage in mfa_usage.txt\n",
    "- After alignment, you can move the unaligned files into a folder called \"unaligned\"\n",
    "\n",
    "# Layout\n",
    "\n",
    "Below is a screenshot of how my folder layout looks after MFA realignment.\n",
    "\n",
    "- aligned : folder with target .wav files and MFA force aligned .TextGrids\n",
    "- clips : folder with original clipped files\n",
    "- unaligned : folder with target .TextGrids before forced alignment\n",
    "\n",
    "- dictionary.txt : dictionary for MFA forced alignment\n",
    "\n",
    "- [FILENAME].csv : .csv file containing metadata\n",
    "- [FILENAME].TextGrid : Praat .TextGrid extracted from ELAN .eaf file\n",
    "\n",
    "<img src='folderlayout.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import read_label from audiolabel to get TextGrid time info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from audiolabel import read_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get preceding and following words for each file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contexts(word_data, phone_data):\n",
    "\n",
    "    '''Takes a word and phone dataframe and updates it for context'''\n",
    "    \n",
    "    # Word context\n",
    "    \n",
    "    prev_words = ['sentence_start']\n",
    "    prev_word_starts = ['sentence_start']\n",
    "    prev_word_ends = ['sentence_start']\n",
    "    \n",
    "    next_words = []\n",
    "    next_word_starts = []\n",
    "    next_word_ends = []\n",
    "    \n",
    "    # Times for previous\n",
    "\n",
    "    for i in range(0, len(word_data)):\n",
    "        \n",
    "        if i != 0:\n",
    "            prev_words.append(word_data.loc[i - 1, 'label'])\n",
    "            prev_word_starts.append(word_data.loc[i - 1, 't1'])\n",
    "            prev_word_ends.append(word_data.loc[i - 1, 't2'])\n",
    "            \n",
    "        if i != len(word_data) - 1:\n",
    "            next_words.append(word_data.loc[i + 1, 'label'])\n",
    "            next_word_starts.append(word_data.loc[i + 1, 't1'])\n",
    "            next_word_ends.append(word_data.loc[i + 1, 't2'])\n",
    "\n",
    "    next_words.append('sentence_end')\n",
    "    next_word_starts.append('sentence_end')\n",
    "    next_word_ends.append('sentence_end')\n",
    "\n",
    "    # Now add lists to datasets\n",
    "    word_data['prev_word'] = prev_words\n",
    "    word_data['prev_word_start'] = prev_word_starts\n",
    "    word_data['prev_word_end'] = prev_word_ends\n",
    "    \n",
    "    word_data['next_word'] = next_words\n",
    "    word_data['next_word_start'] = next_word_starts\n",
    "    word_data['next_word_end'] = next_word_ends\n",
    "    \n",
    "    # Get sentence and get rid of extra whitespaces\n",
    "    sentence = ' '.join((' '.join(word_data['label'].values)).split())\n",
    "    \n",
    "    word_data['sentence'] = sentence\n",
    "    \n",
    "    ###\n",
    "    \n",
    "    phone_data['word'] = phone_data.apply(lambda row : word_data[(word_data['t1'] <= row['t1']) & (word_data['t2'] >= row['t2'])]['label'].item(), axis = 1)\n",
    "    \n",
    "    phone_data['prev_word'] = phone_data.apply(lambda row : word_data[(word_data['t1'] <= row['t1']) & (word_data['t2'] >= row['t2'])]['prev_word'].item(), axis = 1)\n",
    "    phone_data['prev_word_start'] = phone_data.apply(lambda row : word_data[(word_data['t1'] <= row['t1']) & (word_data['t2'] >= row['t2'])]['prev_word_start'].item(), axis = 1)\n",
    "    phone_data['prev_word_end'] = phone_data.apply(lambda row : word_data[(word_data['t1'] <= row['t1']) & (word_data['t2'] >= row['t2'])]['prev_word_end'].item(), axis = 1)\n",
    "    \n",
    "    phone_data['next_word'] = phone_data.apply(lambda row : word_data[(word_data['t1'] <= row['t1']) & (word_data['t2'] >= row['t2'])]['next_word'].item(), axis = 1)\n",
    "    phone_data['next_word_start'] = phone_data.apply(lambda row : word_data[(word_data['t1'] <= row['t1']) & (word_data['t2'] >= row['t2'])]['next_word_start'].item(), axis = 1)\n",
    "    phone_data['next_word_end'] = phone_data.apply(lambda row : word_data[(word_data['t1'] <= row['t1']) & (word_data['t2'] >= row['t2'])]['next_word_end'].item(), axis = 1)\n",
    "    \n",
    "    phone_data['word_start'] = phone_data.apply(lambda row : word_data[(word_data['t1'] <= row['t1']) & (word_data['t2'] >= row['t2'])]['t1'].item(), axis = 1)\n",
    "    phone_data['word_end'] = phone_data.apply(lambda row : word_data[(word_data['t1'] <= row['t1']) & (word_data['t2'] >= row['t2'])]['t2'].item(), axis = 1)\n",
    "    \n",
    "    phone_data['sentence'] = sentence\n",
    "    \n",
    "    # Phone context\n",
    "\n",
    "    prev_phons = ['sentence_start']\n",
    "    prev_phon_starts = ['sentence_start']\n",
    "    prev_phon_ends = ['sentence_start']\n",
    "    \n",
    "    next_phons = []\n",
    "    next_phon_starts = []\n",
    "    next_phon_ends = []\n",
    "\n",
    "    for i in range(0, len(phone_data)):\n",
    "        \n",
    "        if i != 0:\n",
    "            prev_phons.append(phone_data.loc[i - 1, 'label'])\n",
    "            prev_phon_starts.append(phone_data.loc[i - 1, 't1'])\n",
    "            prev_phon_ends.append(phone_data.loc[i - 1, 't2'])\n",
    "            \n",
    "        if i != len(phone_data) - 1:\n",
    "            next_phons.append(phone_data.loc[i + 1, 'label'])\n",
    "            next_phon_starts.append(phone_data.loc[i + 1, 't1'])\n",
    "            next_phon_ends.append(phone_data.loc[i + 1, 't2'])\n",
    "            \n",
    "    next_phons.append('sentence_end')\n",
    "    next_phon_starts.append('sentence_end')\n",
    "    next_phon_ends.append('sentence_end')\n",
    "\n",
    "    phone_data['prev_phon'] = prev_phons\n",
    "    phone_data['prev_phon_start'] = prev_phon_starts\n",
    "    phone_data['prev_phon_end'] = prev_phon_ends\n",
    "    \n",
    "    phone_data['next_phon'] = next_phons\n",
    "    phone_data['next_phon_start'] = next_phon_starts\n",
    "    phone_data['next_phon_end'] = next_phon_ends\n",
    "    \n",
    "    return(word_data, phone_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df = pd.DataFrame()\n",
    "phone_df = pd.DataFrame()\n",
    "\n",
    "for f in glob.glob('*/aligned/*.TextGrid'):\n",
    "    \n",
    "    [sub_wdf, sub_pdf] = read_label(f, ftype = 'praat')\n",
    "    \n",
    "    [updated_wdf, updated_pdf] = get_contexts(sub_wdf, sub_pdf)\n",
    "    \n",
    "    word_df = pd.concat([word_df, updated_wdf])\n",
    "    phone_df = pd.concat([phone_df, updated_pdf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add phoneme and first syllable info\n",
    "\n",
    "# Define syllable\n",
    "def get_firstsyll(word):\n",
    "\n",
    "    vowels = ['a', 'i', 'u', 'e', 'o']\n",
    "\n",
    "    syll = ''\n",
    "\n",
    "    # Flags for onset, nucleus and coda\n",
    "    onset = 'incomplete'\n",
    "    nucleus = 'incomplete'\n",
    "\n",
    "    # If word is 2 for fewer letters long, just take it\n",
    "    if len(word) <= 2:\n",
    "\n",
    "        return(word)\n",
    "\n",
    "    # Keep moving i until we find a vowel\n",
    "    i = 0\n",
    "\n",
    "    # If first sound is vowel, no longer in onset\n",
    "    if word[0] in vowels:\n",
    "        onset = 'complete'\n",
    "\n",
    "    while onset != 'complete':\n",
    "\n",
    "        i += 1\n",
    "\n",
    "        syll += word[i-1]\n",
    "\n",
    "        if word[i] in vowels: \n",
    "\n",
    "            onset = 'complete'\n",
    "\n",
    "    # Look for end of word or coda\n",
    "    while nucleus != 'complete':\n",
    "\n",
    "        i += 1\n",
    "\n",
    "        syll += word[i-1]\n",
    "\n",
    "        if i == len(word):\n",
    "\n",
    "            return(syll)\n",
    "\n",
    "        if word[i] not in vowels:\n",
    "\n",
    "            nucleus = 'complete'\n",
    "\n",
    "    # If sound is n\n",
    "    if word[i] == 'n':\n",
    "\n",
    "        i += 1\n",
    "\n",
    "        # If whole word, then return\n",
    "        if i == len(word):\n",
    "\n",
    "            return(word)\n",
    "\n",
    "        # If next sound is consonant, add\n",
    "        if word[i] not in vowels:\n",
    "\n",
    "            syll += word[i-1]\n",
    "\n",
    "    return(syll)\n",
    "\n",
    "###\n",
    "\n",
    "def findonset(word):\n",
    "    \n",
    "    nuclei = ['a', 'e', 'i', 'o', 'u', 'w', 'j']\n",
    "    \n",
    "    onset = ''\n",
    "    \n",
    "    for phone in word:\n",
    "        \n",
    "        if phone not in nuclei:\n",
    "            \n",
    "            onset += phone\n",
    "        \n",
    "        else: return(onset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "phone_df['onset'] = phone_df['word'].apply(lambda w: findonset(w))\n",
    "phone_df['first_syll'] = phone_df['word'].apply(lambda w: get_firstsyll(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86834"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(phone_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>t1</th>\n",
       "      <th>t2</th>\n",
       "      <th>label</th>\n",
       "      <th>fname</th>\n",
       "      <th>word</th>\n",
       "      <th>prev_word</th>\n",
       "      <th>prev_word_start</th>\n",
       "      <th>prev_word_end</th>\n",
       "      <th>next_word</th>\n",
       "      <th>next_word_start</th>\n",
       "      <th>...</th>\n",
       "      <th>word_end</th>\n",
       "      <th>sentence</th>\n",
       "      <th>prev_phon</th>\n",
       "      <th>prev_phon_start</th>\n",
       "      <th>prev_phon_end</th>\n",
       "      <th>next_phon</th>\n",
       "      <th>next_phon_start</th>\n",
       "      <th>next_phon_end</th>\n",
       "      <th>onset</th>\n",
       "      <th>first_syll</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>sil</td>\n",
       "      <td>YO0010c_yoneshiro_europestory_20130424/aligned...</td>\n",
       "      <td></td>\n",
       "      <td>sentence_start</td>\n",
       "      <td>sentence_start</td>\n",
       "      <td>sentence_start</td>\n",
       "      <td>cigaija</td>\n",
       "      <td>0.09</td>\n",
       "      <td>...</td>\n",
       "      <td>0.09</td>\n",
       "      <td>cigaija kija kirunga qjappari unu munu qbakuda...</td>\n",
       "      <td>sentence_start</td>\n",
       "      <td>sentence_start</td>\n",
       "      <td>sentence_start</td>\n",
       "      <td>CH</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.17</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.09</td>\n",
       "      <td>0.17</td>\n",
       "      <td>CH</td>\n",
       "      <td>YO0010c_yoneshiro_europestory_20130424/aligned...</td>\n",
       "      <td>cigaija</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.09</td>\n",
       "      <td></td>\n",
       "      <td>0.92</td>\n",
       "      <td>...</td>\n",
       "      <td>0.92</td>\n",
       "      <td>cigaija kija kirunga qjappari unu munu qbakuda...</td>\n",
       "      <td>sil</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.09</td>\n",
       "      <td>IY1</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.26</td>\n",
       "      <td>c</td>\n",
       "      <td>ci</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.17</td>\n",
       "      <td>0.26</td>\n",
       "      <td>IY1</td>\n",
       "      <td>YO0010c_yoneshiro_europestory_20130424/aligned...</td>\n",
       "      <td>cigaija</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.09</td>\n",
       "      <td></td>\n",
       "      <td>0.92</td>\n",
       "      <td>...</td>\n",
       "      <td>0.92</td>\n",
       "      <td>cigaija kija kirunga qjappari unu munu qbakuda...</td>\n",
       "      <td>CH</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.17</td>\n",
       "      <td>G</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>c</td>\n",
       "      <td>ci</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>G</td>\n",
       "      <td>YO0010c_yoneshiro_europestory_20130424/aligned...</td>\n",
       "      <td>cigaija</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.09</td>\n",
       "      <td></td>\n",
       "      <td>0.92</td>\n",
       "      <td>...</td>\n",
       "      <td>0.92</td>\n",
       "      <td>cigaija kija kirunga qjappari unu munu qbakuda...</td>\n",
       "      <td>IY1</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.26</td>\n",
       "      <td>AA1</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.39</td>\n",
       "      <td>c</td>\n",
       "      <td>ci</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.32</td>\n",
       "      <td>0.39</td>\n",
       "      <td>AA1</td>\n",
       "      <td>YO0010c_yoneshiro_europestory_20130424/aligned...</td>\n",
       "      <td>cigaija</td>\n",
       "      <td></td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.09</td>\n",
       "      <td></td>\n",
       "      <td>0.92</td>\n",
       "      <td>...</td>\n",
       "      <td>0.92</td>\n",
       "      <td>cigaija kija kirunga qjappari unu munu qbakuda...</td>\n",
       "      <td>G</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>IY1</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.49</td>\n",
       "      <td>c</td>\n",
       "      <td>ci</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     t1    t2 label                                              fname  \\\n",
       "0  0.00  0.09   sil  YO0010c_yoneshiro_europestory_20130424/aligned...   \n",
       "1  0.09  0.17    CH  YO0010c_yoneshiro_europestory_20130424/aligned...   \n",
       "2  0.17  0.26   IY1  YO0010c_yoneshiro_europestory_20130424/aligned...   \n",
       "3  0.26  0.32     G  YO0010c_yoneshiro_europestory_20130424/aligned...   \n",
       "4  0.32  0.39   AA1  YO0010c_yoneshiro_europestory_20130424/aligned...   \n",
       "\n",
       "      word       prev_word prev_word_start   prev_word_end next_word  \\\n",
       "0           sentence_start  sentence_start  sentence_start   cigaija   \n",
       "1  cigaija                             0.0            0.09             \n",
       "2  cigaija                             0.0            0.09             \n",
       "3  cigaija                             0.0            0.09             \n",
       "4  cigaija                             0.0            0.09             \n",
       "\n",
       "  next_word_start  ... word_end  \\\n",
       "0            0.09  ...     0.09   \n",
       "1            0.92  ...     0.92   \n",
       "2            0.92  ...     0.92   \n",
       "3            0.92  ...     0.92   \n",
       "4            0.92  ...     0.92   \n",
       "\n",
       "                                            sentence       prev_phon  \\\n",
       "0  cigaija kija kirunga qjappari unu munu qbakuda...  sentence_start   \n",
       "1  cigaija kija kirunga qjappari unu munu qbakuda...             sil   \n",
       "2  cigaija kija kirunga qjappari unu munu qbakuda...              CH   \n",
       "3  cigaija kija kirunga qjappari unu munu qbakuda...             IY1   \n",
       "4  cigaija kija kirunga qjappari unu munu qbakuda...               G   \n",
       "\n",
       "  prev_phon_start   prev_phon_end next_phon next_phon_start next_phon_end  \\\n",
       "0  sentence_start  sentence_start        CH            0.09          0.17   \n",
       "1             0.0            0.09       IY1            0.17          0.26   \n",
       "2            0.09            0.17         G            0.26          0.32   \n",
       "3            0.17            0.26       AA1            0.32          0.39   \n",
       "4            0.26            0.32       IY1            0.39          0.49   \n",
       "\n",
       "  onset first_syll  \n",
       "0  None             \n",
       "1     c         ci  \n",
       "2     c         ci  \n",
       "3     c         ci  \n",
       "4     c         ci  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phone_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       11066\n",
       "k       8330\n",
       "m       6940\n",
       "b       6498\n",
       "n       5961\n",
       "h       4217\n",
       "t       4054\n",
       "d       3221\n",
       "q       2577\n",
       "tt      2441\n",
       "c       2135\n",
       "s       2108\n",
       "nn      1863\n",
       "nd      1127\n",
       "ns       863\n",
       "qk       840\n",
       "kk       738\n",
       "ts       723\n",
       "nt       529\n",
       "qs       515\n",
       "qt       484\n",
       "nm       477\n",
       "ngg      437\n",
       "qh       433\n",
       "g        376\n",
       "qn       308\n",
       "p        304\n",
       "qz       284\n",
       "qm       254\n",
       "nb       230\n",
       "qb       211\n",
       "qd       192\n",
       "nk       186\n",
       "qts      182\n",
       "qg       152\n",
       "qc       106\n",
       "ng        74\n",
       "ss        60\n",
       "qr        54\n",
       "mb        11\n",
       "qy        11\n",
       "qtt       10\n",
       "nts       10\n",
       "pp         8\n",
       "qf         4\n",
       "cc         3\n",
       "qnn        3\n",
       "ttt        3\n",
       "Name: onset, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phone_df['onset'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_df.to_csv('../data/words.csv', index = False)\n",
    "phone_df.to_csv('../data/phones.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get target words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Raksit/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n",
      "/Users/Raksit/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "target_words = word_df[(word_df['label'].str.len() > 1) & (word_df['label'].str.contains('^[tkcpmn(ts)]'))]\n",
    "\n",
    "target_phones_all = phone_df[(phone_df['word'].str.len() > 1) & (phone_df['word'].str.contains('^[tkcpmn(ts)]')) & (phone_df['label'].isin(['T', 'K', 'CH', 'P', 'M', 'N'])) & (phone_df['word_start'] == phone_df['t1'])]\n",
    "\n",
    "target_phones_all['onset'] = target_phones_all['word'].apply(lambda w: findonset(w))\n",
    "target_phones_all['first_syll'] = target_phones_all['word'].apply(lambda w: get_firstsyll(w))\n",
    "\n",
    "# Reorder columns\n",
    "col_order = ['fname', 'onset', 'label', 't1', 't2',\n",
    "             'word', 'word_start', 'word_end',\n",
    "             'first_syll', 'sentence',\n",
    "             'prev_phon', 'prev_phon_start', 'prev_phon_end', 'next_phon', 'next_phon_start', 'next_phon_end', \n",
    "             'prev_word', 'prev_word_start', 'prev_word_end', 'next_word', 'next_word_start', 'next_word_end'\n",
    "             ]\n",
    "\n",
    "target_phones_all = target_phones_all[col_order]\n",
    "\n",
    "# Rename t1 and t2 to more useful names\n",
    "target_phones_all = target_phones_all.rename(columns = {'t1':'phon_start', 't2':'phon_end'})\n",
    "\n",
    "# Isolate phones that are preceded by vowels\n",
    "target_phones = target_phones_all[target_phones_all['prev_phon'].isin(['AA1', 'IY1', 'UW1', 'EY1', 'OW1', 'N'])].reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'YO0001',\n",
       " 'YO0002',\n",
       " 'YO0004',\n",
       " 'YO0005',\n",
       " 'YO0006',\n",
       " 'YO0008c',\n",
       " 'YO0008d',\n",
       " 'YO0008e',\n",
       " 'YO0009b',\n",
       " 'YO0010a',\n",
       " 'YO0010b',\n",
       " 'YO0010c',\n",
       " 'YO0011a',\n",
       " 'YO0011b',\n",
       " 'YO0012b',\n",
       " 'YO0014b',\n",
       " 'YO0015',\n",
       " 'YO0016',\n",
       " 'YO0017a',\n",
       " 'YO0017b',\n",
       " 'YO0019a',\n",
       " 'YO0019b',\n",
       " 'YO0020a',\n",
       " 'YO0020b',\n",
       " 'YO0020c',\n",
       " 'YO0021',\n",
       " 'YO0024b',\n",
       " 'YO0025a'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(target_phones.fname.apply(lambda x: x.split('_')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_phones.to_csv('../data/target_phones.csv', index = False)\n",
    "target_phones_all.to_csv('../data/target_phones_all.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_phones_all.groupby(['onset'])['word'].value_counts().reset_index(name = 'count').to_csv('../data/word_count_all.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_phones.groupby(['onset'])['word'].value_counts().reset_index(name = 'count').to_csv('../data/word_count.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_phones.groupby(['onset'])['first_syll'].value_counts().reset_index(name = 'count').to_csv('../data/first_syll_count.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_phones_all.groupby(['onset'])['first_syll'].value_counts().reset_index(name = 'count').to_csv('../data/first_syll_count_all.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
